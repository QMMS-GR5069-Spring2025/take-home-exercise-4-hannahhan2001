{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "280e8d3c-fb35-4431-bd8c-4876778f52d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, count\n",
    "from pyspark.sql.types import IntegerType, DoubleType\n",
    "import mlflow\n",
    "import mlflow.spark\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "from pyspark.ml.regression import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d86301ad-ff58-41be-b647-f202ee1fe857",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install mysql-connector-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a7d5b72-c58e-4934-a80d-f77bc7af71c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install mlflow mysql-connector-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36cfed4b-9ccb-461a-95da-638cb59cae4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_laps = spark.read.csv(\"s3://columbia-gr5069-main/raw/lap_times.csv\", header=True,)\n",
    "df_results = spark.read.csv('s3://columbia-gr5069-main/raw/results.csv', header = True)\n",
    "df_drivers = spark.read.csv(\"s3://columbia-gr5069-main/raw/drivers.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc58963f-8532-4546-bd62-9d9db8f5df29",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Select only needed columns from each DataFrame\n",
    "laps_sel = df_laps.select(\"raceId\", \"driverId\", \"lap\", \"position\", \"time\", \"milliseconds\")\n",
    "results_sel = df_results.select(\"raceId\", \"driverId\", \"constructorId\", \"grid\", \"positionOrder\", \"points\", \"statusId\")\n",
    "drivers_sel = df_drivers.select(\"driverId\", \"driverRef\", \"code\", \"forename\", \"surname\", \"nationality\")\n",
    "\n",
    "# Rename columns to avoid confusion \n",
    "results_sel = results_sel.withColumnRenamed(\"positionOrder\", \"finalPosition\")\n",
    "drivers_sel = drivers_sel.withColumnRenamed(\"nationality\", \"driverNationality\")\n",
    "\n",
    "# Join laps with results\n",
    "df_laps_results = laps_sel.join(results_sel, on=[\"raceId\", \"driverId\"], how=\"inner\")\n",
    "\n",
    "# Join with drivers\n",
    "df_final = df_laps_results.join(drivers_sel, on=\"driverId\", how=\"inner\")\n",
    "\n",
    "# Show schema and a few rows\n",
    "df_final.printSchema()\n",
    "df_final.display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8ff944f-bfc9-411f-84fa-f4593a9ba246",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Filter nulls if needed\n",
    "df_model = df_final.dropna(subset=['milliseconds'])\n",
    "\n",
    "# Select features and label\n",
    "feature_cols = ['grid', 'position', 'points', 'lap']  # You can add more\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol='features')\n",
    "\n",
    "# Define model\n",
    "rf = RandomForestRegressor(labelCol='milliseconds', featuresCol='features')\n",
    "\n",
    "# Create pipeline\n",
    "pipeline = Pipeline(stages=[assembler, rf])\n",
    "\n",
    "# Train/test split\n",
    "train, test = df_model.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1934b70-bfaf-46be-9247-64631f22efad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "import mlflow\n",
    "import mlflow.spark\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Cast the columns to numeric types\n",
    "train = train.withColumn(\"grid\", col(\"grid\").cast(\"double\"))\n",
    "train = train.withColumn(\"position\", col(\"position\").cast(\"double\"))\n",
    "train = train.withColumn(\"points\", col(\"points\").cast(\"double\"))\n",
    "train = train.withColumn(\"lap\", col(\"lap\").cast(\"double\"))\n",
    "train = train.withColumn(\"milliseconds\", col(\"milliseconds\").cast(\"double\"))\n",
    "\n",
    "test = test.withColumn(\"grid\", col(\"grid\").cast(\"double\"))\n",
    "test = test.withColumn(\"position\", col(\"position\").cast(\"double\"))\n",
    "test = test.withColumn(\"points\", col(\"points\").cast(\"double\"))\n",
    "test = test.withColumn(\"lap\", col(\"lap\").cast(\"double\"))\n",
    "test = test.withColumn(\"milliseconds\", col(\"milliseconds\").cast(\"double\"))\n",
    "\n",
    "with mlflow.start_run(run_name=\"RF_Model_LapTime\") as run:\n",
    "    model = pipeline.fit(train)\n",
    "    predictions = model.transform(test)\n",
    "\n",
    "    # Evaluate\n",
    "    evaluator = RegressionEvaluator(labelCol=\"milliseconds\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "    rmse = evaluator.evaluate(predictions)\n",
    "    \n",
    "    # Log model and params\n",
    "    mlflow.log_param(\"features\", feature_cols)\n",
    "    mlflow.log_metric(\"rmse\", rmse)\n",
    "\n",
    "    # Log the pipeline model\n",
    "    mlflow.spark.log_model(model, \"random-forest-pipeline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1aaece86-dc14-4dba-8472-3dfe89a6eb80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "conn = mysql.connector.connect(\n",
    "    host='hh3098-gr5069.ccqalx6jsr2n.us-east-1.rds.amazonaws.com',\n",
    "    user='admin',\n",
    "    password='011102Hannah'\n",
    ")\n",
    "\n",
    "# Create a cursor object\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Execute the SQL command to create a database\n",
    "cursor.execute(\"CREATE DATABASE IF NOT EXISTS gr5069\")\n",
    "cursor.execute(\"USE gr5069\")\n",
    "\n",
    "# Close the cursor and connection\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bcc54936-14ef-4a74-98e8-cd75cc3c9404",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Select relevant columns and rename prediction\n",
    "pred_df = predictions.select(\"raceId\", \"driverId\", \"milliseconds\", col(\"prediction\").alias(\"predicted_ms\"))\n",
    "\n",
    "# Optional: cast columns to match MySQL types (if needed)\n",
    "pred_df = pred_df.withColumn(\"milliseconds\", col(\"milliseconds\").cast(\"double\")) \\\n",
    "                 .withColumn(\"predicted_ms\", col(\"predicted_ms\").cast(\"double\"))\n",
    "\n",
    "# Write to MySQL\n",
    "pred_df.write.format('jdbc').options(\n",
    "    url='jdbc:mysql://hh3098-gr5069.ccqalx6jsr2n.us-east-1.rds.amazonaws.com/gr5069',\n",
    "    driver='com.mysql.jdbc.Driver', \n",
    "    dbtable='lap_time_predictions',\n",
    "    user='admin',\n",
    "    password='011102Hannah'\n",
    ").mode('overwrite').save()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "231c5642-5ab0-4bb5-8afb-947fdd82d86e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "[20 pts] Create two (2) new tables in your own database where you'll store the predictions from each model for this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1c5daf5-6bc4-41af-8240-bb7eaace3739",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# model a\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "\n",
    "rf = RandomForestRegressor(featuresCol=\"features\", labelCol=\"milliseconds\")\n",
    "pipeline_rf = Pipeline(stages=[assembler, rf])\n",
    "model_rf = pipeline_rf.fit(train)\n",
    "predictions_model_a = model_rf.transform(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b69f2946-7606-4866-8681-39725b195955",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# model b\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"milliseconds\")\n",
    "pipeline_lr = Pipeline(stages=[assembler, lr])\n",
    "model_lr = pipeline_lr.fit(train)\n",
    "predictions_model_b = model_lr.transform(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af6864a2-e957-4082-b559-1f410c447fd1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "predictions_model_a = predictions_model_a.select(\n",
    "    col(\"raceId\").cast(\"int\"),\n",
    "    col(\"driverId\").cast(\"int\"),\n",
    "    col(\"milliseconds\").cast(\"double\").alias(\"actual_ms\"),\n",
    "    col(\"prediction\").cast(\"double\").alias(\"predicted_ms\")\n",
    ")\n",
    "\n",
    "predictions_model_b = predictions_model_b.select(\n",
    "    col(\"raceId\").cast(\"int\"),\n",
    "    col(\"driverId\").cast(\"int\"),\n",
    "    col(\"milliseconds\").cast(\"double\").alias(\"actual_ms\"),\n",
    "    col(\"prediction\").cast(\"double\").alias(\"predicted_ms\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "beb3a775-8d1f-4a0a-a8a1-a6e63afe1448",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write Model A predictions\n",
    "predictions_model_a.write.format('jdbc').options(\n",
    "    url='jdbc:mysql://hh3098-gr5069.ccqalx6jsr2n.us-east-1.rds.amazonaws.com/gr5069',\n",
    "    driver='com.mysql.jdbc.Driver',\n",
    "    dbtable='model_a_predictions',\n",
    "    user='admin',\n",
    "    password='011102Hannah'\n",
    ").mode('overwrite').save()\n",
    "\n",
    "# Write Model B predictions\n",
    "predictions_model_b.write.format('jdbc').options(\n",
    "    url='jdbc:mysql://hh3098-gr5069.ccqalx6jsr2n.us-east-1.rds.amazonaws.com/gr5069',\n",
    "    driver='com.mysql.jdbc.Driver',\n",
    "    dbtable='model_b_predictions',\n",
    "    user='admin',\n",
    "    password='011102Hannah'\n",
    ").mode('overwrite').save()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b8d1d3da-9f3f-4476-9611-75294ea3699d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "[30 pts] Build two (2) predictive models using MLflow, logging hyperparameters, the model itself, four metrics, and two artifcats. Submit submit your MLflow experiments as part of your assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd430e8e-9b4a-4bf0-adb9-7a80d71adee6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def log_model_with_mlflow(model, model_name, params, train_df, test_df, label_col=\"milliseconds\"):\n",
    "    from matplotlib import pyplot as plt\n",
    "    import pandas as pd\n",
    "\n",
    "    assembler = VectorAssembler(inputCols=[\"grid\", \"position\", \"lap\"], outputCol=\"features\")\n",
    "    pipeline = Pipeline(stages=[assembler, model])\n",
    "\n",
    "    with mlflow.start_run(run_name=model_name):\n",
    "        fitted_model = pipeline.fit(train_df)\n",
    "        predictions = fitted_model.transform(test_df)\n",
    "\n",
    "        # Evaluators\n",
    "        metrics = {\n",
    "            \"rmse\": RegressionEvaluator(labelCol=label_col, predictionCol=\"prediction\", metricName=\"rmse\").evaluate(predictions),\n",
    "            \"mae\": RegressionEvaluator(labelCol=label_col, predictionCol=\"prediction\", metricName=\"mae\").evaluate(predictions),\n",
    "            \"r2\": RegressionEvaluator(labelCol=label_col, predictionCol=\"prediction\", metricName=\"r2\").evaluate(predictions),\n",
    "            \"mse\": RegressionEvaluator(labelCol=label_col, predictionCol=\"prediction\", metricName=\"mse\").evaluate(predictions),\n",
    "        }\n",
    "\n",
    "        # Log hyperparameters\n",
    "        for k, v in params.items():\n",
    "            mlflow.log_param(k, v)\n",
    "\n",
    "        # Log metrics\n",
    "        for k, v in metrics.items():\n",
    "            mlflow.log_metric(k, v)\n",
    "\n",
    "        # Log model\n",
    "        mlflow.spark.log_model(fitted_model, \"model\")\n",
    "\n",
    "        # Artifact 1: Save predictions as CSV\n",
    "        pred_pd = predictions.select(\"prediction\", label_col).toPandas()\n",
    "        pred_path = os.path.join(tempfile.mkdtemp(), f\"{model_name}_predictions.csv\")\n",
    "        pred_pd.to_csv(pred_path, index=False)\n",
    "        mlflow.log_artifact(pred_path, artifact_path=\"predictions\")\n",
    "\n",
    "        # Artifact 2: Save plot\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.scatter(pred_pd[label_col], pred_pd[\"prediction\"], alpha=0.5)\n",
    "        ax.plot([pred_pd[label_col].min(), pred_pd[label_col].max()],\n",
    "                [pred_pd[label_col].min(), pred_pd[label_col].max()],\n",
    "                color='red', linestyle='--')\n",
    "        ax.set_xlabel(\"Actual\")\n",
    "        ax.set_ylabel(\"Predicted\")\n",
    "        ax.set_title(f\"{model_name} Predictions\")\n",
    "        plot_path = os.path.join(tempfile.mkdtemp(), f\"{model_name}_scatter.png\")\n",
    "        plt.savefig(plot_path)\n",
    "        mlflow.log_artifact(plot_path, artifact_path=\"plots\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b7d4efd-2074-43ef-ae3a-24d7755a4684",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "import os\n",
    "import tempfile\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "\n",
    "rf_model = RandomForestRegressor(featuresCol=\"features\", labelCol=\"milliseconds\", numTrees=50, maxDepth=5)\n",
    "log_model_with_mlflow(rf_model, \"RandomForestModel\", {\"numTrees\": 50, \"maxDepth\": 5}, train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4c3304d-5862-4913-bc91-ed45309368cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "lr_model = LinearRegression(featuresCol=\"features\", labelCol=\"milliseconds\", maxIter=10, regParam=0.1)\n",
    "log_model_with_mlflow(lr_model, \"LinearRegressionModel\", {\"maxIter\": 10, \"regParam\": 0.1}, train, test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f55aa0c8-1980-4f97-b211-7f4ae7b84dec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "take-home-exercise-4-hannahhan2001",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
